{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753130ac",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e894acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, \n",
    "                                      Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042129a9",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from MELD dataset\n",
    "print(\"Loading MELD labels...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load text embeddings for alignment (needed to extract utterance IDs)\n",
    "pkl_path = 'MELD.Raw/text_emotion.pkl'\n",
    "if not os.path.exists(pkl_path):\n",
    "    raise FileNotFoundError(f\"Text embeddings not found at: {pkl_path}\")\n",
    "\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    text_data = pickle.load(f)\n",
    "\n",
    "train_text_dict, val_text_dict, test_text_dict = text_data\n",
    "print(f\"✓ Text embeddings loaded: {len(train_text_dict)} train, {len(val_text_dict)} val, {len(test_text_dict)} test dialogues\")\n",
    "\n",
    "# Load labels from CSV files\n",
    "base_path = 'MELD.Raw'\n",
    "train_labels_df = pd.read_csv(os.path.join(base_path, 'train_sent_emo.csv'))\n",
    "dev_labels_df = pd.read_csv(os.path.join(base_path, 'dev_sent_emo.csv'))\n",
    "test_labels_df = pd.read_csv(os.path.join(base_path, 'test_sent_emo.csv'))\n",
    "\n",
    "print(f\"✓ Labels loaded: {len(train_labels_df)} train, {len(dev_labels_df)} val, {len(test_labels_df)} test utterances\")\n",
    "\n",
    "# Create label lookup dictionaries\n",
    "def create_label_dict(df):\n",
    "    \"\"\"Create dictionary mapping (dialogue_id, utterance_id) -> emotion\"\"\"\n",
    "    return {(row['Dialogue_ID'], row['Utterance_ID']): row['Emotion'] \n",
    "            for _, row in df.iterrows()}\n",
    "\n",
    "train_label_dict = create_label_dict(train_labels_df)\n",
    "val_label_dict = create_label_dict(dev_labels_df)\n",
    "test_label_dict = create_label_dict(test_labels_df)\n",
    "\n",
    "# Extract utterances with labels (for text alignment)\n",
    "def extract_utterances_with_labels(text_dict, label_dict):\n",
    "    \"\"\"Extract text features and labels for valid utterances\"\"\"\n",
    "    X_text_list, y_list, dia_ids, utt_ids = [], [], [], []\n",
    "    \n",
    "    for dia_id in text_dict.keys():\n",
    "        text_features = text_dict[dia_id]\n",
    "        for utt_idx in range(text_features.shape[0]):\n",
    "            if np.any(text_features[utt_idx]):  # Non-zero utterance\n",
    "                key = (int(dia_id), utt_idx)\n",
    "                if key in label_dict:\n",
    "                    X_text_list.append(text_features[utt_idx])\n",
    "                    y_list.append(label_dict[key])\n",
    "                    dia_ids.append(dia_id)\n",
    "                    utt_ids.append(utt_idx)\n",
    "    \n",
    "    return np.array(X_text_list), y_list, dia_ids, utt_ids\n",
    "\n",
    "print(\"\\nExtracting utterances with labels...\")\n",
    "X_text_train, train_emotions, train_dia_ids, train_utt_ids = extract_utterances_with_labels(train_text_dict, train_label_dict)\n",
    "X_text_val, val_emotions, val_dia_ids, val_utt_ids = extract_utterances_with_labels(val_text_dict, val_label_dict)\n",
    "X_text_test, test_emotions, test_dia_ids, test_utt_ids = extract_utterances_with_labels(test_text_dict, test_label_dict)\n",
    "\n",
    "print(f\"  Train: {len(X_text_train)} utterances\")\n",
    "print(f\"  Val: {len(X_text_val)} utterances\")\n",
    "print(f\"  Test: {len(X_text_test)} utterances\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "all_emotions = train_emotions + val_emotions + test_emotions\n",
    "le.fit(all_emotions)\n",
    "\n",
    "y_train = le.transform(train_emotions)\n",
    "y_val = le.transform(val_emotions)\n",
    "y_test = le.transform(test_emotions)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"\\n✓ Labels encoded: {num_classes} emotion classes\")\n",
    "print(f\"  Classes: {list(le.classes_)}\")\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53067f07",
   "metadata": {},
   "source": [
    "## 3. Load Spectrogram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed spectrograms\n",
    "# Note: Spectrograms were generated using audio_to_spectrogram.ipynb\n",
    "# They are already aligned with the labels (same indices)\n",
    "\n",
    "spectrogram_path = 'MELD_spectrograms'\n",
    "\n",
    "print(f\"Loading spectrogram data from: {spectrogram_path}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_audio_train = np.load(os.path.join(spectrogram_path, 'X_audio_train.npy'))\n",
    "X_audio_val = np.load(os.path.join(spectrogram_path, 'X_audio_val.npy'))\n",
    "X_audio_test = np.load(os.path.join(spectrogram_path, 'X_audio_test.npy'))\n",
    "\n",
    "print(f\"✓ Spectrogram data loaded successfully!\")\n",
    "print(f\"  Train: {X_audio_train.shape}\")\n",
    "print(f\"  Val: {X_audio_val.shape}\")\n",
    "print(f\"  Test: {X_audio_test.shape}\")\n",
    "\n",
    "# Verify alignment with labels\n",
    "assert X_audio_train.shape[0] == y_train.shape[0], \"Train size mismatch!\"\n",
    "assert X_audio_val.shape[0] == y_val.shape[0], \"Val size mismatch!\"\n",
    "assert X_audio_test.shape[0] == y_test.shape[0], \"Test size mismatch!\"\n",
    "\n",
    "print(f\"\\n✓ Spectrograms are properly aligned with labels\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d645c16",
   "metadata": {},
   "source": [
    "## 4. Build 2D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build 2D CNN for spectrogram-based emotion classification\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of spectrogram input (height, width, channels)\n",
    "        num_classes: Number of emotion classes\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential(name='CNN_Spectrogram_Model')\n",
    "    \n",
    "    # Conv Block 1\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Conv Block 2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Conv Block 3\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Conv Block 4\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the CNN model\n",
    "cnn_input_shape = X_audio_train.shape[1:]\n",
    "cnn_model = build_2d_cnn_model(cnn_input_shape, num_classes)\n",
    "\n",
    "print(\"2D CNN Model Architecture:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721ecfe",
   "metadata": {},
   "source": [
    "## 5. Train 2D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalanced data\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights_array))\n",
    "\n",
    "# Cap weights to prevent instability\n",
    "for k, v in class_weights_dict.items():\n",
    "    class_weights_dict[k] = min(v, 3.0)\n",
    "\n",
    "print(f\"Class weights: {class_weights_dict}\")\n",
    "\n",
    "# Compile CNN model\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "cnn_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint('best_cnn_spectrogram_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"\\nTraining 2D CNN model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train the model\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_audio_train, y_train_cat,\n",
    "    validation_data=(X_audio_val, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=cnn_callbacks,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ 2D CNN model training complete!\")\n",
    "print(f\"Best Validation Accuracy: {max(cnn_history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf282b0",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CNN training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(cnn_history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(cnn_history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0].set_title('2D CNN Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(cnn_history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(cnn_history.history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('2D CNN Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Accuracy: {cnn_history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {cnn_history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c4426",
   "metadata": {},
   "source": [
    "## 7. Evaluate 2D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc024b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN model on test set\n",
    "print(\"2D CNN Model Evaluation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_probs = cnn_model.predict(X_audio_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_loss, cnn_test_accuracy = cnn_model.evaluate(X_audio_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('2D CNN Model - Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8429c",
   "metadata": {},
   "source": [
    "## 8. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance analysis\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(\"PER-CLASS PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate precision, recall, and F1 scores per class\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=None, labels=range(num_classes)\n",
    ")\n",
    "\n",
    "# Create detailed dataframe\n",
    "per_class_df = pd.DataFrame({\n",
    "    'Emotion': le.classes_,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nDetailed Per-Class Metrics:\")\n",
    "print(per_class_df.to_string(index=False))\n",
    "\n",
    "# Visualize per-class F1 scores\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(le.classes_))\n",
    "\n",
    "bars = ax.bar(x, f1, color='#e74c3c', alpha=0.8)\n",
    "ax.set_xlabel('Emotion Class')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('Per-Class F1-Score - 2D CNN Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(le.classes_, rotation=45, ha='right')\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "print(f\"\\nBest performing emotions:\")\n",
    "best_emotions = per_class_df.nlargest(3, 'F1-Score')\n",
    "for _, row in best_emotions.iterrows():\n",
    "    print(f\"  {row['Emotion']}: F1={row['F1-Score']:.4f} (support={int(row['Support'])})\")\n",
    "\n",
    "print(f\"\\nWorst performing emotions:\")\n",
    "worst_emotions = per_class_df.nsmallest(3, 'F1-Score')\n",
    "for _, row in worst_emotions.iterrows():\n",
    "    print(f\"  {row['Emotion']}: F1={row['F1-Score']:.4f} (support={int(row['Support'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558d80e",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample spectrograms from each emotion class\n",
    "print(\"Sample Spectrograms by Emotion Class\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, emotion in enumerate(le.classes_):\n",
    "    # Find first sample of this emotion\n",
    "    emotion_idx = np.where(y_true == i)[0][0]\n",
    "    sample_spec = X_audio_test[emotion_idx]\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    axes[i].imshow(sample_spec[:, :, 0], aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[i].set_title(f'{emotion.capitalize()}')\n",
    "    axes[i].set_xlabel('Time')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f36be9",
   "metadata": {},
   "source": [
    "## 10. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_save_path = 'best_cnn_spectrogram_model.keras'\n",
    "cnn_model.save(model_save_path)\n",
    "print(f\"✓ Model saved to: {model_save_path}\")\n",
    "\n",
    "# Save predictions for further analysis\n",
    "predictions_dict = {\n",
    "    'true_labels': y_true,\n",
    "    'predicted_labels': y_pred,\n",
    "    'predicted_probabilities': y_pred_probs,\n",
    "    'emotion_classes': le.classes_,\n",
    "    'test_accuracy': cnn_test_accuracy\n",
    "}\n",
    "\n",
    "np.save('cnn_spectrogram_predictions.npy', predictions_dict)\n",
    "print(f\"✓ Predictions saved to: cnn_spectrogram_predictions.npy\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: 2D CNN for Spectrogram-based Emotion Recognition\")\n",
    "print(f\"Input Shape: {X_audio_train.shape[1:]}\")\n",
    "print(f\"Number of Classes: {num_classes}\")\n",
    "print(f\"Training Samples: {X_audio_train.shape[0]}\")\n",
    "print(f\"Validation Samples: {X_audio_val.shape[0]}\")\n",
    "print(f\"Test Samples: {X_audio_test.shape[0]}\")\n",
    "print(f\"\\nFinal Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(cnn_history.history['val_accuracy']):.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
